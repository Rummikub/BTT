{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Timeseries\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Example timestamps (replace with your actual timestamps)\n",
    "timestamps = [\n",
    "    \"2022-01-15 08:00:00\",\n",
    "    \"2022-01-15 12:30:00\",\n",
    "    \"2022-01-15 18:45:00\"\n",
    "]\n",
    "\n",
    "# Convert timestamps to Unix timestamps\n",
    "numeric_values = [datetime.strptime(ts, \"%Y-%m-%d %H:%M:%S\").timestamp() for ts in timestamps]\n",
    "\n",
    "# Calculate average\n",
    "average_numeric = sum(numeric_values) / len(numeric_values)\n",
    "\n",
    "# Convert average back to timestamp\n",
    "average_timestamp = datetime.fromtimestamp(average_numeric).strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using numpy\n",
    "import numpy as np\n",
    "\n",
    "# Example timestamps (replace with your actual timestamps)\n",
    "timestamps = [\n",
    "    \"08:00:00\",\n",
    "    \"12:30:00\",\n",
    "    \"18:45:00\"\n",
    "]\n",
    "\n",
    "# Convert timestamps to total seconds\n",
    "total_seconds = [int(ts.split(\":\")[0])*3600 + int(ts.split(\":\")[1])*60 + int(ts.split(\":\")[2]) for ts in timestamps]\n",
    "\n",
    "# Calculate average using np.mean()\n",
    "average_seconds = np.mean(total_seconds)\n",
    "\n",
    "# Convert average back to timestamp\n",
    "average_timestamp = \"{:02}:{:02}:{:02}\".format(int(average_seconds // 3600), int((average_seconds % 3600) // 60), int(average_seconds % 60))\n",
    "\n",
    "print(\"Average Timestamp:\", average_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Data\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# defining a single generation block function\n",
    "def FC_Layer_blockGen(input_dim, output_dim):\n",
    "    single_block = nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    return single_block\n",
    "    \n",
    "# DEFINING THE GENERATOR\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "            nn.Tanh()  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "#defining a single discriminattor block       \n",
    "def FC_Layer_BlockDisc(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4)\n",
    "    )\n",
    "    \n",
    "# Defining the discriminator\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "        \n",
    "#Defining training parameters\n",
    "batch_size = 128\n",
    "num_epochs = 500\n",
    "lr = 0.0002\n",
    "num_features = 6\n",
    "latent_dim = 20\n",
    "\n",
    "# MODEL INITIALIZATION\n",
    "generator = Generator(noise_dim, num_features)\n",
    "discriminator = Discriminator(num_features)\n",
    "\n",
    "# LOSS FUNCTION AND OPTIMIZERS\n",
    "criterion = nn.BCELoss()\n",
    "gen_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "disc_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING DATA\n",
    "file_path = 'SamplingData7.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "X = data.values\n",
    "X_normalized = torch.FloatTensor((X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) * 2 - 1)\n",
    "real_data = X_normalized\n",
    "\n",
    "#Creating a dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe.values.astype(float)\n",
    "        self.labels = dataframe.values.astype(float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'input': torch.tensor(self.data[idx]),\n",
    "            'label': torch.tensor(self.labels[idx])\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "# Create an instance of the dataset\n",
    "dataset = MyDataset(data)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)\n",
    "\n",
    "pretrained = False\n",
    "if pretrained:\n",
    "    pre_dict = torch.load('pretrained_model.pth')\n",
    "    generator.load_state_dict(pre_dict['generator'])\n",
    "    discriminator.load_state_dict(pre_dict['discriminator'])\n",
    "else:\n",
    "    # Apply weight initialization\n",
    "    generator = generator.apply(weights_init)\n",
    "    discriminator = discriminator.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_freq = 100\n",
    "\n",
    "latent_dim =20\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        real_data_batch = batch['input']\n",
    "        # Train discriminator on real data\n",
    "        real_labels = torch.FloatTensor(np.random.uniform(0.9, 1.0, (batch_size, 1)))\n",
    "        disc_optimizer.zero_grad()\n",
    "        output_real = discriminator(real_data_batch)\n",
    "        loss_real = criterion(output_real, real_labels)\n",
    "        loss_real.backward()\n",
    "\n",
    "        # Train discriminator on generated data\n",
    "        fake_labels = torch.FloatTensor(np.random.uniform(0, 0.1, (batch_size, 1)))\n",
    "        noise = torch.FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim)))\n",
    "        generated_data = generator(noise)\n",
    "        output_fake = discriminator(generated_data.detach())\n",
    "        loss_fake = criterion(output_fake, fake_labels)\n",
    "        loss_fake.backward()\n",
    "\n",
    "        disc_optimizer.step()\n",
    "\n",
    "        # Train generator \n",
    "        valid_labels = torch.FloatTensor(np.random.uniform(0.9, 1.0, (batch_size, 1)))\n",
    "        gen_optimizer.zero_grad()\n",
    "        output_g = discriminator(generated_data)\n",
    "        loss_g = criterion(output_g, valid_labels)\n",
    "        loss_g.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch}, D Loss Real: {loss_real.item()}, D Loss Fake: {loss_fake.item()}, G Loss: {loss_g.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Generate synthetic data \n",
    "synthetic_data = generator(torch.FloatTensor(np.random.normal(0, 1, (real_data.shape[0], noise_dim))))\n",
    "\n",
    "# Plot the results\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "fig.suptitle('Real and Synthetic Data Distributions', fontsize=16)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        sns.histplot(synthetic_data[:, i * 3 + j].detach().numpy(), bins=50, alpha=0.5, label='Synthetic Data', ax=axs[i, j], color='blue')\n",
    "        sns.histplot(real_data[:, i * 3 + j].numpy(), bins=50, alpha=0.5, label='Real Data', ax=axs[i, j], color='orange')\n",
    "        axs[i, j].set_title(f'Parameter {i * 3 + j + 1}', fontsize=12)\n",
    "        axs[i, j].set_xlabel('Value')\n",
    "        axs[i, j].set_ylabel('Frequency')\n",
    "        axs[i, j].legend()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create a 2x3 grid of subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Comparison of Real and Synthetic Data', fontsize=16)\n",
    "\n",
    "# Define parameter names\n",
    "param_names = ['Parameter 1', 'Parameter 2', 'Parameter 3', 'Parameter 4', 'Parameter 5', 'Parameter 6']\n",
    "\n",
    "# Scatter plots for each parameter\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        param_index = i * 3 + j\n",
    "        sns.scatterplot(real_data[:, 0].numpy(), real_data[:, param_index].numpy(), label='Real Data', alpha=0.5, ax=axs[i, j])\n",
    "        sns.scatterplot(synthetic_data[:, 0].detach().numpy(), synthetic_data[:, param_index].detach().numpy(), label='Generated Data', alpha=0.5, ax=axs[i, j])\n",
    "        axs[i, j].set_title(param_names[param_index], fontsize=12)\n",
    "        axs[i, j].set_xlabel(f'Real Data - {param_names[param_index]}')\n",
    "        axs[i, j].set_ylabel(f'Real Data - {param_names[param_index]}')\n",
    "        axs[i, j].legend()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* I've been researching the complexity of generating synthetic data. While creating it from scratch is challenging, I've explored GAN methods, specifically using PyTorch. I plan to experiment with this approach, although its applicability to our dataset is yet to be determined.\"\n",
    "\n",
    "* \"Setting a threshold for isolation forest has been a challenge. Following Ken's suggestion, I've removed the contamination level and approached the code from different angles to address this difficulty.\"\n",
    "\n",
    "* \"Although we temporarily dropped the duration column from our dataset, I'm considering using the mean value of duration as a feature for isolation forest. This might reveal interesting insights, and I'll be working on implementing this.\"\n",
    "\n",
    "* \"I've recognized the importance of setting hypotheses and considering potential attacking scenarios for generating synthetic data. I've summarized some scenarios relevant to our dataset that we should be mindful of.\"\n",
    "\n",
    "* \"Regarding task prioritization, I'm feeling a bit lost about where to start. While I understand we need to address all discussed tasks, I would appreciate guidance on the order of priority. Any insights on what should be tackled first would be helpful.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge 이해하기\n",
    "\n",
    "Isolation FOrest 직접 구현\n",
    "\n",
    "합성데이터 직접 구현\n",
    "\n",
    "https://towardsdatascience.com/generative-ai-synthetic-data-generation-with-gans-using-pytorch-2e4dde8a17dd\n",
    "\n",
    "https://www.youtube.com/watch?v=Gg0gH3-Q4Wk&ab_channel=Unit8\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###01/19 Update\n",
    "\n",
    "https://blog.eunsukim.me/posts/what-is-accuracy-recall-precision-and-f1-score \n",
    "\n",
    "https://www.youtube.com/watch?v=puVdwi5PjVA&ab_channel=%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90%EC%82%B0%EC%97%85%EA%B2%BD%EC%98%81%EA%B3%B5%ED%95%99%EB%B6%80DSBA%EC%97%B0%EA%B5%AC%EC%8B%A4\n",
    " \n",
    "\n",
    "https://www.youtube.com/watch?v=VZWQfQHsGGY&ab_channel=%E2%80%8D%EA%B9%80%EC%84%B1%EB%B2%94%5B%EA%B5%90%EC%88%98%2F%EC%82%B0%EC%97%85%EA%B2%BD%EC%98%81%EA%B3%B5%ED%95%99%EB%B6%80%5D \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Step 1: Train the Isolation Forest Model\n",
    "iso_forest = IsolationForest()\n",
    "iso_forest.fit(X)\n",
    "\n",
    "# Step 2: Make Predictions\n",
    "predictions = iso_forest.predict(X)\n",
    "\n",
    "# Step 3: Calculate Metrics\n",
    "# Assuming your true labels are stored in 'y_true'\n",
    "f1 = f1_score(y_true, predictions)\n",
    "recall = recall_score(y_true, predictions)\n",
    "precision = precision_score(y_true, predictions)\n",
    "\n",
    "print(f'F1-score: {f1}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'Precision: {precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal threshold\n",
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "\n",
    "# Assuming 'y_true' and 'predictions' are defined\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, predictions)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "print(f'Optimal Threshold (F1): {optimal_threshold}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Threshold for Precision\n",
    "optimal_threshold_precision = thresholds[np.argmax(precision)]\n",
    "print(f'Optimal Threshold (Precision): {optimal_threshold_precision}')\n",
    "\n",
    "# Optimal Threshold for Recall\n",
    "optimal_threshold_recall = thresholds[np.argmax(recall)]\n",
    "print(f'Optimal Threshold (Recall): {optimal_threshold_recall}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update 2/15/2024\n",
    "\n",
    "from pyod.models.iforest import IForest\n",
    "\n",
    "\n",
    "iforest=IForest(n_estimators=300, contatimation=0.05, random_state=10)\n",
    "\n",
    "iforest.fit(X)\n",
    "\n",
    "# outlier labels\n",
    "labels = iforest.fit_predict(X)\n",
    "\n",
    "outliers = X[labels==1]\n",
    "\n",
    "print(outliers.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#calculate probabilities\n",
    "# Alternative to isolating outliers with contatimation = outlier probability\n",
    "iforest = IForest(random_state=10).fit(X)\n",
    "\n",
    "# Calculate probabilities\n",
    "probs = iforest.predict_proba(X)\n",
    "\n",
    "# Extract the probabilities for outliers\n",
    "outlier_probs = probs[:,1]\n",
    "\n",
    "# Filter for when the probability is higher than 70%\n",
    "outliers = X[outlier_probs>.7]\n",
    "\n",
    "print(len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "# Instantiate KNN and fit to females\n",
    "knn = KNN(contamination=0.005, n_neighbors=20, n_jobs=-1)\n",
    "knn.fit(X)\n",
    "\n",
    "# Create a boolean index that checks for outliers\n",
    "is_outlier = knn.labels_ == 1\n",
    "\n",
    "# Isolate the outliers\n",
    "outliers = X[is_outlier]\n",
    "\n",
    "print(len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate a KNN with 20 neighbors and fit to `X`\n",
    "knn = KNN(n_neighbors = 20, n_jobs=-1)\n",
    "knn.fit(X)\n",
    "\n",
    "# Calculate probabilities\n",
    "probs = knn.predict_proba(X)\n",
    "\n",
    "# Create a boolean mask\n",
    "is_outlier = probs[:,1] > .55\n",
    "# Use the boolean mask to fier the outliers\n",
    "outliers = X[is_outlier]\n",
    "\n",
    "print(len(outliers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_outlier_classifier(model, data, threshold=.75):\n",
    "    model.fit(data)\n",
    "\n",
    "    probs = model.predict_proba(data)\n",
    "    inliers = data[probs[:, 1] <= threshold]\n",
    "\n",
    "    return inliers\n",
    "\n",
    "def evaluate_regressor(inliers):\n",
    "    X, y = inliers.drop(\"weightkg\", axis=1), inliers[['weightkg']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10, train_size=0.8)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    preds = lr.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "\n",
    "    return round(rmse, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning n_neighbors\n",
    "\n",
    "# Create a list of values for n_neigbors\n",
    "n_neighbors = [5,10,20]\n",
    "scores = dict()\n",
    "\n",
    "for k in n_neighbors:\n",
    "    # Instantiate KNN with the current k\n",
    "    knn = KNN(n_neighbors=k, n_jobs=-1)\n",
    "    \n",
    "    # Find the inliers with the current KNN\n",
    "    inliers = evaluate_outlier_classifier(knn, females_transformed, .50)\n",
    "    \n",
    "    # Calculate and store RMSE into scores\n",
    "    scores[k] = evaluate_regressor(inliers)\n",
    "    \n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning aggregation method\n",
    "'''\n",
    "Once the optimal number of neighbors is found, it's time to tune the distance aggregating method. If n_neighbors is 10, each datapoint will have ten distance measurements to its nearest neighbors. KNN uses three methods to aggregate those distances: largest, mean, and median.\n",
    "'''\n",
    "\n",
    "n_neighbors = [5, 20]\n",
    "methods = ['largest', 'mean', 'median']\n",
    "scores = dict()\n",
    "\n",
    "for k, m in product(n_neighbors,methods):\n",
    "    # Create a KNN instance\n",
    "    knn = KNN(n_neighbors=k,method=m,n_jobs=-1)\n",
    "    \n",
    "    # Find the inliers with the current KNN\n",
    "    inliers = evaluate_outlier_classifier(knn,females_transformed, .5)\n",
    "\n",
    "    # Calculate and store RMSE into scores\n",
    "    scores[(k, m)] = evaluate_regressor(inliers)\n",
    "    \n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOF\n",
    "# Import LOF from its relevant module\n",
    "from pyod.models.lof import LOF\n",
    "\n",
    "# Instantiate LOF and fit to females_transformed\n",
    "lof = LOF(contamination=.003, n_jobs=-1)\n",
    "lof.fit(X_transformed)\n",
    "\n",
    "# Create a boolean index that checks for outliers\n",
    "is_outlier = lof.labels_ == 1\n",
    "\n",
    "# Isolate the outliers\n",
    "outliers = X_transformed[is_outlier]\n",
    "\n",
    "print(len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof = LOF(n_neighbors=20)\n",
    "lof.fit(females_transformed)\n",
    "\n",
    "# Calculate probabilities\n",
    "probs = lof.predict_proba(females_transformed)\n",
    "\n",
    "# Create a boolean mask\n",
    "is_outlier = probs[:,1]>.5\n",
    "\n",
    "# Use the boolean mask to filter the outliers\n",
    "outliers = females_transformed[is_outlier]\n",
    "\n",
    "print(len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize a StandardScaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Extract feature and target arrays\n",
    "X = X.drop('target',axis=1)\n",
    "y = X[['target']]\n",
    "\n",
    "# Fit/transform X\n",
    "X_transformed = ss.fit_transform(X)\n",
    "\n",
    "# Fit/transform X but preserve the column names\n",
    "X.loc[:,:] = ss.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUntileTransformer\n",
    "'''\n",
    "Standardization is prone to the same pitfalls as z-scores. Both use mean and standardization in their calculations, which makes them highly sensitive to extreme values.\n",
    "\n",
    "To get around this problem, you should use QuantileTransformer which uses quantiles. Quantiles of a distribution stay the same regardless of the magnitude of outliers.\n",
    "\n",
    "You should use StandardScaler when the data is normally distributed (which can be checked with a histogram). For other distributions, QuantileTransformer is a better choice.\n",
    "\n",
    "'''\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# Instantiate an instance that casts to normal\n",
    "qt = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "# Fit and transform the feature array\n",
    "X.loc[:,:] = qt.fit_transform(X)\n",
    "\n",
    "# Plot a histogram of palm length\n",
    "plt.hist(X['target'], color='red')\n",
    "\n",
    "plt.xlabel(\"Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling Timeseries\n",
    "\n",
    "# Convert the Date column to DateTime\n",
    "apple['Date'] = pd.to_datetime(apple['Date'])\n",
    "\n",
    "# Create a column for the day of the week\n",
    "apple['day_of_week'] = apple['Date'].dt.day_of_week\n",
    "\n",
    "# Create a column for the month\n",
    "apple['month'] = apple['Date'].dt.month\n",
    "# Create a column for the day of the month\n",
    "apple['day_of_month'] =apple['Date'].dt.day\n",
    " \n",
    "\n",
    "print(apple[['day_of_week', 'month', 'day_of_month']])\n",
    "\n",
    "# Convert the Date column to DateTime\n",
    "apple['Date'] = pd.to_datetime(apple['Date'])\n",
    "\n",
    "# Create a column for the day of the week\n",
    "apple['day_of_week'] = apple['Date'].dt.day_of_week\n",
    "\n",
    "# Create a column for the month\n",
    "apple['month'] = apple['Date'].dt.month\n",
    "# Create a column for the day of the month\n",
    "apple['day_of_month'] =apple['Date'].dt.day\n",
    " \n",
    "\n",
    "print(apple[['day_of_week', 'month', 'day_of_month']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Apple stocks dataset with a proper DatatimeIndex\n",
    "apple = pd.read_csv('aapl.csv',parse_dates=['Date'],index_col='Date')\n",
    "# Create three new features from the DatetimeIndex\n",
    "apple['day_of_week'] = apple.index.day_of_week\n",
    "apple['month'] = apple.index.month\n",
    "apple['day_of_month'] = apple.index.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decomposition\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Create a DecomposeResult object and plot seasonality\n",
    "results = seasonal_decompose(apple['Volume'][\"2010\": \"2012\"], period=365)\n",
    "\n",
    "results.seasonal.plot(color=\"red\", figsize=(12, 4))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = seasonal_decompose(apple['Volume'], period=365)\n",
    "\n",
    "# Extract and reshape the residuals\n",
    "residuals = results.resid\n",
    "residuals = residuals.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating, thresholding and probs\n",
    "\n",
    "# Find the mean across rows\n",
    "mean_probs = np.mean(probability_scores,axis=1)\n",
    "\n",
    "# Create a boolean mask that uses a 75% threshold\n",
    "is_outlier = mean_probs > .75\n",
    "\n",
    "# Use the mask to filter outliers from apple\n",
    "outliers = apple[is_outlier]\n",
    "\n",
    "print(len(outliers))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
