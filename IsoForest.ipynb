{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Timeseries\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Example timestamps (replace with your actual timestamps)\n",
    "timestamps = [\n",
    "    \"2022-01-15 08:00:00\",\n",
    "    \"2022-01-15 12:30:00\",\n",
    "    \"2022-01-15 18:45:00\"\n",
    "]\n",
    "\n",
    "# Convert timestamps to Unix timestamps\n",
    "numeric_values = [datetime.strptime(ts, \"%Y-%m-%d %H:%M:%S\").timestamp() for ts in timestamps]\n",
    "\n",
    "# Calculate average\n",
    "average_numeric = sum(numeric_values) / len(numeric_values)\n",
    "\n",
    "# Convert average back to timestamp\n",
    "average_timestamp = datetime.fromtimestamp(average_numeric).strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using numpy\n",
    "import numpy as np\n",
    "\n",
    "# Example timestamps (replace with your actual timestamps)\n",
    "timestamps = [\n",
    "    \"08:00:00\",\n",
    "    \"12:30:00\",\n",
    "    \"18:45:00\"\n",
    "]\n",
    "\n",
    "# Convert timestamps to total seconds\n",
    "total_seconds = [int(ts.split(\":\")[0])*3600 + int(ts.split(\":\")[1])*60 + int(ts.split(\":\")[2]) for ts in timestamps]\n",
    "\n",
    "# Calculate average using np.mean()\n",
    "average_seconds = np.mean(total_seconds)\n",
    "\n",
    "# Convert average back to timestamp\n",
    "average_timestamp = \"{:02}:{:02}:{:02}\".format(int(average_seconds // 3600), int((average_seconds % 3600) // 60), int(average_seconds % 60))\n",
    "\n",
    "print(\"Average Timestamp:\", average_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Data\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# defining a single generation block function\n",
    "def FC_Layer_blockGen(input_dim, output_dim):\n",
    "    single_block = nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    return single_block\n",
    "    \n",
    "# DEFINING THE GENERATOR\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "            nn.Tanh()  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "#defining a single discriminattor block       \n",
    "def FC_Layer_BlockDisc(input_dim, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4)\n",
    "    )\n",
    "    \n",
    "# Defining the discriminator\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "        \n",
    "        \n",
    "#Defining training parameters\n",
    "batch_size = 128\n",
    "num_epochs = 500\n",
    "lr = 0.0002\n",
    "num_features = 6\n",
    "latent_dim = 20\n",
    "\n",
    "# MODEL INITIALIZATION\n",
    "generator = Generator(noise_dim, num_features)\n",
    "discriminator = Discriminator(num_features)\n",
    "\n",
    "# LOSS FUNCTION AND OPTIMIZERS\n",
    "criterion = nn.BCELoss()\n",
    "gen_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "disc_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING DATA\n",
    "file_path = 'SamplingData7.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "X = data.values\n",
    "X_normalized = torch.FloatTensor((X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) * 2 - 1)\n",
    "real_data = X_normalized\n",
    "\n",
    "#Creating a dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe.values.astype(float)\n",
    "        self.labels = dataframe.values.astype(float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'input': torch.tensor(self.data[idx]),\n",
    "            'label': torch.tensor(self.labels[idx])\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "# Create an instance of the dataset\n",
    "dataset = MyDataset(data)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)\n",
    "\n",
    "pretrained = False\n",
    "if pretrained:\n",
    "    pre_dict = torch.load('pretrained_model.pth')\n",
    "    generator.load_state_dict(pre_dict['generator'])\n",
    "    discriminator.load_state_dict(pre_dict['discriminator'])\n",
    "else:\n",
    "    # Apply weight initialization\n",
    "    generator = generator.apply(weights_init)\n",
    "    discriminator = discriminator.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_freq = 100\n",
    "\n",
    "latent_dim =20\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        real_data_batch = batch['input']\n",
    "        # Train discriminator on real data\n",
    "        real_labels = torch.FloatTensor(np.random.uniform(0.9, 1.0, (batch_size, 1)))\n",
    "        disc_optimizer.zero_grad()\n",
    "        output_real = discriminator(real_data_batch)\n",
    "        loss_real = criterion(output_real, real_labels)\n",
    "        loss_real.backward()\n",
    "\n",
    "        # Train discriminator on generated data\n",
    "        fake_labels = torch.FloatTensor(np.random.uniform(0, 0.1, (batch_size, 1)))\n",
    "        noise = torch.FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim)))\n",
    "        generated_data = generator(noise)\n",
    "        output_fake = discriminator(generated_data.detach())\n",
    "        loss_fake = criterion(output_fake, fake_labels)\n",
    "        loss_fake.backward()\n",
    "\n",
    "        disc_optimizer.step()\n",
    "\n",
    "        # Train generator \n",
    "        valid_labels = torch.FloatTensor(np.random.uniform(0.9, 1.0, (batch_size, 1)))\n",
    "        gen_optimizer.zero_grad()\n",
    "        output_g = discriminator(generated_data)\n",
    "        loss_g = criterion(output_g, valid_labels)\n",
    "        loss_g.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch}, D Loss Real: {loss_real.item()}, D Loss Fake: {loss_fake.item()}, G Loss: {loss_g.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Generate synthetic data \n",
    "synthetic_data = generator(torch.FloatTensor(np.random.normal(0, 1, (real_data.shape[0], noise_dim))))\n",
    "\n",
    "# Plot the results\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8))\n",
    "fig.suptitle('Real and Synthetic Data Distributions', fontsize=16)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        sns.histplot(synthetic_data[:, i * 3 + j].detach().numpy(), bins=50, alpha=0.5, label='Synthetic Data', ax=axs[i, j], color='blue')\n",
    "        sns.histplot(real_data[:, i * 3 + j].numpy(), bins=50, alpha=0.5, label='Real Data', ax=axs[i, j], color='orange')\n",
    "        axs[i, j].set_title(f'Parameter {i * 3 + j + 1}', fontsize=12)\n",
    "        axs[i, j].set_xlabel('Value')\n",
    "        axs[i, j].set_ylabel('Frequency')\n",
    "        axs[i, j].legend()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create a 2x3 grid of subplots\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Comparison of Real and Synthetic Data', fontsize=16)\n",
    "\n",
    "# Define parameter names\n",
    "param_names = ['Parameter 1', 'Parameter 2', 'Parameter 3', 'Parameter 4', 'Parameter 5', 'Parameter 6']\n",
    "\n",
    "# Scatter plots for each parameter\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        param_index = i * 3 + j\n",
    "        sns.scatterplot(real_data[:, 0].numpy(), real_data[:, param_index].numpy(), label='Real Data', alpha=0.5, ax=axs[i, j])\n",
    "        sns.scatterplot(synthetic_data[:, 0].detach().numpy(), synthetic_data[:, param_index].detach().numpy(), label='Generated Data', alpha=0.5, ax=axs[i, j])\n",
    "        axs[i, j].set_title(param_names[param_index], fontsize=12)\n",
    "        axs[i, j].set_xlabel(f'Real Data - {param_names[param_index]}')\n",
    "        axs[i, j].set_ylabel(f'Real Data - {param_names[param_index]}')\n",
    "        axs[i, j].legend()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* I've been researching the complexity of generating synthetic data. While creating it from scratch is challenging, I've explored GAN methods, specifically using PyTorch. I plan to experiment with this approach, although its applicability to our dataset is yet to be determined.\"\n",
    "\n",
    "* \"Setting a threshold for isolation forest has been a challenge. Following Ken's suggestion, I've removed the contamination level and approached the code from different angles to address this difficulty.\"\n",
    "\n",
    "* \"Although we temporarily dropped the duration column from our dataset, I'm considering using the mean value of duration as a feature for isolation forest. This might reveal interesting insights, and I'll be working on implementing this.\"\n",
    "\n",
    "* \"I've recognized the importance of setting hypotheses and considering potential attacking scenarios for generating synthetic data. I've summarized some scenarios relevant to our dataset that we should be mindful of.\"\n",
    "\n",
    "* \"Regarding task prioritization, I'm feeling a bit lost about where to start. While I understand we need to address all discussed tasks, I would appreciate guidance on the order of priority. Any insights on what should be tackled first would be helpful.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge 이해하기\n",
    "\n",
    "Isolation FOrest 직접 구현\n",
    "\n",
    "합성데이터 직접 구현\n",
    "\n",
    "https://towardsdatascience.com/generative-ai-synthetic-data-generation-with-gans-using-pytorch-2e4dde8a17dd\n",
    "\n",
    "https://www.youtube.com/watch?v=Gg0gH3-Q4Wk&ab_channel=Unit8\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
